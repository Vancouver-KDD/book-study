# CHAPTER 13: DESIGN A SEARCH AUTOCOMPLETE SYSTEM

Design a search autocomplete system, also called â€œdesign top kâ€ or â€œdesign top k most searched queriesâ€.

![fg13-1](img/fg13-1.jpg)

## Step 1 - Understand the problem and establish design scope

###  Requirements

```
Here is a summary of the requirements:
  â€¢ Fast response time: / ë¹ ë¥¸ì‘ë‹µ
      As a user types a search query, autocomplete suggestions must show up fast enough. 
      Facebookâ€™s autocomplete system -> 100 milliseconds. 
      Otherwise -> stuttering.
  â€¢ Relevant: / ì—°ê´€ì„±, ì í•©í•œ ì¶œë ¥
      Autocomplete suggestions should be relevant to the search term.
  â€¢ Sorted: / ì •ë ¬, ì¸ê¸°ë„ ë“±ì˜ ìˆœìœ„ ëª¨ë¸
      Results returned by the system must be sorted by popularity or other ranking models.
  â€¢ Scalable: í™•ì¥ì„±, íŠ¸ë˜í”½ ê°ë‹¹
      The system can handle high traffic volume.
  â€¢ Highly available: ê°€ìš©ì„±, ì¥ì•  ì‹œ/ì œì•½ì¡°ê±´ ì‹œ ê·¸ë˜ë„ ë™ì‘
      The system should remain available and accessible when part of the 
  system is offline, slows down, or experiences unexpected network errors.
```

### Back of the envelope estimation
```
â€¢ Assume 10 million daily active users (DAU).
â€¢ An average person performs 10 searches per day.
â€¢ 20 bytes of data per query string:
  â€¢ Assume we use ASCII character encoding. 1 character = 1 byte
  â€¢ Assume a query = 4 words & 1 word = 5 characters, on average.
  â€¢ That is 4 x 5 x 1 byte = 20 bytes per query.
â€¢ For every character entered into the search box, a client sends a request to the backend for autocomplete suggestions.
  On average, 20 requests are sent for each search query.
  For example, the following 6 requests are sent to the backend by the time you finish typing 
      â€œdinnerâ€.
      search?q=d
      search?q=di
      search?q=din
      search?q=dinn
      search?q=dinne
      search?q=dinner
â€¢ ~24,000 query per second (QPS) = 10,000,000 users * 10 queries / day * 20 characters / 24 hours / 3600 seconds. -> 23,184.148
â€¢ Peak QPS = QPS * 2 = ~48,000
â€¢ Assume 20% of the daily queries are new.
    10 million * 10 queries / day * 20 byte per query * 20% = 0.4 GB.
    This means 0.4GB of new data is added to storage daily.  
```

## Step 2 - Propose high-level design and get buy-in

At the high-level,
> â€¢ *Data gathering service*
```
: It gathers user input queries and aggregates them in real-time. ì‚¬ìš©ìì…ë ¥ ì§ˆì˜ ì‹¤ì‹œê°„ ìˆ˜ì§‘
Real-time processing is not practical for large data sets;
however, it is a good starting point. We will explore a more realistic solution in deep dive.
```

> â€¢ *Query service* :
```
Given a search query or prefix, return 5 most frequently searched terms./ 5ê°œ ì¸ê¸°ì§ˆì˜ì–´ ë¦¬í„´
```

### Data gathering service
```
Assume a frequency table (the query string / frequency)  Figure 13-2.
empty ->  â€œtwitchâ€ ->  â€œtwitterâ€ -> â€œtwitter,â€ -> â€œtwilloâ€ sequentially.
```
![fg13-2](img/fg13-2.jpg)

### Query service
```
Assume a frequency table Table 13-1.
Two fields.
â€¢ Query: it stores the query string.
â€¢ Frequency: it represents the number of times a query has been searched.
```
![t13-1](img/t13-1.jpg)

```
For example, Figure 13-3
When a user types â€œtwâ€ in the search box, the following top 5 searched queries are displayed based on Table 13-1
```
![fg13-3](img/fg13-3.jpg)

```
SQL query -> To get top 5 frequently searched queries
```
![fg13-4](img/fg13-4.jpg)

```
ğŸŒ When the data set is small, This is an acceptable solution when the data set is small.
ğŸ˜± When it is large, accessing the database becomes a bottleneck. 
```

## Step 3 - Design deep dive
Components & Optimizations 
```
â€¢ Trie data structure - íŠ¸ë¼ì´ ìë£Œêµ¬ì¡° ì†Œê°œ
â€¢ Data gathering service - ì´ë¥¼ ë‹¤ì‹œ ì ìš©
â€¢ Query service - ì´ë¥¼ ë‹¤ì‹œ ì ìš©
â€¢ Trie operations
â€¢ Scale the storage
```

### (1) Trie data structure

```
Relational databases are used for storage in the high-level design. 
However, fetching the top 5 search queries from a relational database is inefficient.

ğŸ”¥The data structure trie (prefix tree) is used to overcome the problem.
(ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ì˜ ë³‘ëª©í˜„ìƒ í•œê°œë¥¼ The data structure trie (prefix tree)ë¡œ í•´ê²°í•œë‹¤.)

As trie data structure is crucial for the system
Therfore, understanding the basic trie data structure is essential before going further.

> An overview of the trie data structure / How to optimize the basic trie to improve response time

The main idea of trie consists of the following:
  â€¢ A trie is a tree-like data structure. íŠ¸ë¦¬ í˜•íƒœì˜ ìë£Œêµ¬ì¡°
  â€¢ The root represents an empty string.  ë£¨íŠ¸ ë…¸ë“œëŠ” ë¹ˆ ë¬¸ìì—´
  â€¢ Each node stores a character and has 26 children, one for each possible character. 1ê°œ ê¸€ìì™€ 26ê°œ ìì‹ë…¸ë“œ
    (To save space, we do not draw empty links.)
  â€¢ Each tree node represents a single word or a prefix string.
```

Figure 13-5 : A trie with search queries(highlighted) â€œtreeâ€, â€œtryâ€, â€œtrueâ€, â€œtoyâ€, â€œwishâ€, â€œwinâ€. 
![fg13-5](img/fg13-5.jpg)

Table 13-2: To support sorting by frequency, frequency info is added
![t13-1](img/t13-2.jpg)

![fg13-6](img/fg13-6.jpg)


**Some terms** 
```
  â€¢ p: length of a prefix
  â€¢ n: total number of nodes in a trie
  â€¢ c: number of children of a given node
```

**How does autocomplete work with trie?**
```
Top k most searched queries
  1. Find the prefix.
      (Time complexity: O(p) p: length of a prefix)
  2. Traverse the subtree from the prefix node to get all valid children. ê·¸ë¡œë¶€í„° ìœ íš¨ë…¸ë“œë“¤ì„ ê±°ì³ ìì‹ë…¸ë“œ íƒìƒ‰
      A child is valid if it can form a valid query string.
      (Time complexity: O(c) c: number of children of a given node)
  3. Sort the children and get top k. Time complexity: O(clogc)
```

Example (Figure 13-7)
```
Assume k equals to 2 and a user types â€œbeâ€ in the search box. The algorithm works as follows:
  â€¢ Step 1: Find the prefix node â€œbeâ€.
  â€¢ Step 2: Traverse the subtree to get all valid children. N
            Nodes [bee: 20], [beer: 10], [best: 35], [bet: 29] are valid.
  â€¢ Step 3: Sort the children and get top 2.
            [best: 35] and [bet: 29] are the top 2 queries with prefix â€œtrâ€.
```
![fg13-7](img/fg13-7.jpg)

```
The time complexity of this algorithm is the sum of time spent on each step mentioned above: 
  -> O(p) + O(c) + O(clogc)
The above algorithm is straightforward.
However, it is too slowğŸ˜± -> (worst-case scenario) the entire trie traverse

ğŸ™‹â€â™‚ï¸ Below are two optimizations:
  a. Limit the max length of a prefix
  b. Cache top search queries at each node 
```

#### a. Limit the max length of a prefix (prefix ìµœëŒ€ê¸¸ì´ì œí•œ 50)
```
Users rarely type a long search query into the search box. 
Thus, it is safe to say p is a small integer number, say 50. 
If we limit the length of a prefix,
  the time complexity for â€œFind the prefixâ€: O(p) -> ğŸŒO(small constant), aka O(1).
```
#### b. Cache top search queries at each node (ì¸ê¸°ê²€ìƒ‰ì–´ ìºì‹œí™”)
```
Trade off: ìƒìœ„ ë…¸ë“œì—ì„œ ì¸ê¸°ê²€ìƒ‰ì–´ ìºì‹œí™”  Time O(p) -> O(1)ğŸŒ But, space is needed ğŸ˜±
To avoid traversing the whole trie, we store top k most frequently used queries at each node.
Since 5 to 10 autocomplete suggestions are enough for users, k is a relatively small number. 
In our specific case, only the top 5 search queries are cached.
By caching top search queries at every node, we significantly reduce the time complexity to retrieve the top 5 queries.

ğŸ¥µ However, this design requires a lot of space to store top queries at every node.
            Trading space for time is well worth it as fast response time is very important.
```
Example - Cache top search queries  
![fg13-8](img/fg13-8.jpg)

```
Optimized TC :
  1. Find the prefix node. Time complexity: O(1)
  2. Return top k. Since top k queries are cached, the time complexity for this step is O(1). 

Now, our algorithm takes ğŸ”¥only O(1) to fetch top k queries.
```


### (2) Data gathering service

```
In our previous design, It was a real-time service.

Not practical with two reasons:
  â€¢ Users may enter billions of queries per day. Updating the trie on every query significantly slows down the query service.
    ìˆ˜ì²œë§Œ queryì˜ trieì—…ë°ì´íŠ¸ì— ë”°ë¥¸ ì‹¬ê°í•œ ì†ë„ ì €í•˜
  â€¢ Top suggestions may not change much once the trie is built. Thus, it is unnecessary to update the trie frequently.
    ì¸ê¸°ê²€ìƒ‰ì–´ ìˆœìœ„ê°€ ì‹¤ì‹œê°„ ë°”ë€” í•„ìš”ê°€ ì—†ë‹¤(ì˜ ì•ˆë°”ë€œ)

To design a scalable data gathering service, we examine where data comes from and how data is used.

> Twitter(Real-time applications) - up to date autocomplete suggestions
However,
> Google keywords - not  a daily basis change.

data gathering serviceì˜ use casesëŠ” ë‹¤ë¥´ì§€ë§Œ the underlying foundationëŠ” ê°™ë‹¤.

ì¦‰ Analytics or logging services.

```

#### *Redesigned* Data gathering service (Figure 13-9) 
No real-time service. 

![fg13-9](img/fg13-9.jpg)

a. Analytics Logs:
```
  It stores raw data about search queries.
  Logs: Append-only, not indexed.
  ë°ì´í„° ë¶„ì„ ì„œë¹„ìŠ¤ ë¡œê·¸: ì›ë³¸ ë°ì´í„° ë³´ê´€(ê²€ìƒ‰ì°½ì— ì…ë ¥ëœ)
  ì‹ ê·œ ë°ì´í„°ê°€ ì¶”ê°€ / ìˆ˜ì •(X) / not indexed
```

**Example: Log File**

![t13-3](img/t13-3.jpg)

b. Aggregators(ì·¨í•©ì„œë²„):
```
  - The size of analytics logs is usually very large, and data is not in the right format. (ë¶„ì„ë°ì´í„° ë³´í†µ ëŒ€ìš©ëŸ‰ & ë‹¤ì–‘í•œ í˜•ì‹)
  - We need to aggregate data so it can be easily processed by our system. (ëª©ì ì— ë”°ë¼ ì†ì‰¬ìš´ ê°€ê³µì„ ìœ„í•œ ì·¨í•©ìš”)
  - Depending on the use case, we may aggregate data differently.
    For Real-time applications(Twitter) : shorter time interval as real-time results are important. 
  - On the other hand, aggregating data less frequently, say once per week, might be good enough for many use cases. (ì¸í„°ë·° ì‹œ ì‹¤ì‹œê°„ ì·¨í•©? ì£¼ê¸°ì  ì·¨í•©? í™•ì¸ ì¤‘ìš”)
  - During an interview session, verify whether real-time results are important. We assume trie is rebuilt weekly. (ì£¼ë‹¹ ì·¨í•©ìœ¼ë¡œ ê°€ì •í•œë‹¤)
```


c. Aggregated Data(Table 13-4):
```
  ì·¨í•©ëœ ë°ì´í„°ê°€ ìˆê²Œë˜ê³ ....
  Time: the start time of a week.
  Frequency: the sum of the occurrences for the corresponding query in that week.
```

![t13-4](img/t13-4.jpg)

d. Workers(ì‘ì—…ì„œë²„):
```
  ê·¸ ì·¨í•©ëœ ë°ì´í„°ë¡œ ì‘ì—…..
  - Workers are a set of servers that perform #asynchronous# jobs at regular intervals. (weekly ë¹„ë™ê¸°ìˆ˜í–‰ì„œë²„)
  - They build the trie data structure and store it in Trie DB.(Trie DBì— ë§ê²Œ ë°ì´í„°êµ¬ì¡° ìƒì„± ë° ì €ì¥)
```

e. Trie Cache:
```
  Trie Cache is a distributed cache system that keeps trie in memory for fast read.(ë¶„ì‚° ìºì‹œ ì‹œìŠ¤í…œë¡œ ë¹ ë¥¸ ì½ê¸°ì—°ì‚°ìˆ˜í–‰)
  It takes a weekly snapshot of the DB. (ì£¼ë³„ ìŠ¤ëƒ…ìƒ· ê°±ì‹ )
```

f. Trie DB:
```
  Trie DB is the persistent storage. ì˜êµ¬ì €ì¥
  <Two options>
  1. Document store(ì§ë ¬í™”ì €ì¥): (MongoDB... / XML, YAML, JSON..., etc formats) weeklyë² ì´íŠ¸ tri ì§ë ¬í™”ì €ì¥.
                 Since a new trie is built weekly, we can periodically take a snapshot of it, serialize it, and store the serialized data in the database.
                      
  2. Key-value store (í•´ì‹œí…Œì´ë¸”í˜•íƒœì €ì¥ Chapter 6):
                 í•´ì‹œ í…Œì´ë¸” í˜•íƒœë¡œ ë³€í™˜ ì €ì¥
                  â€¢ [key]: every prefix in the trie -> be
                  â€¢ [value]: Data on each trie node -> [be: 15, bee:20, beer:10, best:35]
                Example (figure 13-10)   
```
![fg13-10](img/fg13-10.jpg)



### (3) Query service

![fg13-11](img/fg13-11.jpg)
```
ì¸ê¸° ê²€ìƒ‰ì–´ ë‹¤ì„¯ ê°œ DBë¡œ ë¶€í„° fetch
In the high-level design, the query service calls the database directly to fetch the top 5 results. 

Figure 13-11: the improved design as the previous design is inefficient.

  1. A search query is sent to the load balancer. (Queryê°€ ë¡œë“œë²¨ëŸ°ì„œë¡œ ì „ì†¡)
  2. The load balancer routes the request to API servers.(ë¡œë“œë°¸ëŸ°ì„œ ê·¸ Query API ì„œë²„ë¡œ ì „ì†¡)
  3. API servers get trie data from Trie Cache and construct autocomplete suggestions for the client.
      (APIëŠ” cacheì°¸ì¡° ìš”ì²­ì— ëŒ€í•œ ìë™ì™„ì„± ê²€ìƒ‰ì–´ ì‘ë‹µ êµ¬ì„±)
  4. In case the data is not in Trie Cache, we replenish data back to the cache. (íŠ¸ë¼ì´ìºì‹œì— ì—†ì„ë• DBì„œ ê°€ì ¸ì™€ì„œ ìºì‹œì— ë”í•¨)

  This way, all subsequent requests for the same prefix are returned from the cache. ì´ ë°©ì‹ìœ¼ë¡œ ì°¨í›„ìš”ì²­ì€ ìºì‹œë¡œë¶€í„° ë¦¬í„´
  A cache miss(ìºì‹œ ë¶€ì ì¤‘ <-> cache hit) can happen when a cache server is out of memory or offline.
     (Then, data should be replenished from the DB back to the cache. DBë¡œ ë¶€í„° cacheì— data ì¶©ë‹¹)
```


** - Query service requires lightning-fast speed **

<Three proposed optimizations>

(a) AJAX request 
```
For web applications, browsers usually send AJAX requests to fetch autocomplete results.
The main benefit of AJAX is that sending/receiving a request/response does not refresh the whole web page.
```

(b) Browser caching (i.e. Google search engine).
```
For many applications, autocomplete search suggestions may not change much within a short time.
Thus, autocomplete suggestions can be saved in browser cache to allow subsequent requests to get results from the cache directly.

Figure 13-12: the response header when you type â€œsystem design interviewâ€ on the Google search engine.
As you can see, Google caches the results in the browser for 1 hour.
Please note: â€œprivateâ€ in cache-control means results are intended for a single user and must not be cached by a shared cache.
             â€œmax-age=3600â€ means the cache is valid for 3600 seconds, aka, an hour.
```
![fg13-12](img/fg13-12.jpg)

(c) Data sampling (Nê°œì˜ ìš”ì²­ ì¤‘ 1ê°œë§Œ ë¡œê¹…ê¸°ë¡)
```
For a large-scale system, logging every search query requires a lot of processing power and storage.
Data sampling is important.
For instance, only 1 out of every N requests is logged by the system.
```

### (4) Trie operations
Trie is a core component of the autocomplete system. 

> Create / Update / Delete

#### Create
```
Trie is created by workers using aggregated data. The source of data is from Analytics Log/DB.
  (Trie ìƒì„± - worker ì„œë²„ ë‹´ë‹¹/  Analytics Log/DBë‚˜ ë°ì´í„°ë² ì´ìŠ¤ ì·¨í•©ë°ì´í„° ì´ìš©)
```

#### Update
```
Two ways
- Option 1: Update the trie weekly. (ë§¤ì£¼ í•œ ë²ˆ ê°±ì‹ ë°©ì•ˆ)  Once a new trie is created, the new trie replaces the old one. 
- Option 2: Update individual trie node directly.
            í•´ë‹¹ ë°©ì‹ì€ ëŠë¦¬ì§€ë§Œ íŠ¸ë¼ì´ ì‚¬ì´ì¦ˆê°€ ì‘ë‹¤ë©´ ì ìš©í•´ë´„ì§í•˜ë‹¤( ì±…ì—ì„œëŠ” ëŠë ¤ì„œ ê³ ë ¤ì¹œì•Šì•˜ìŒ)

When we update a trie node, its ancestors all the way up to the root must be updated because ancestors store top queries of children.
(ì£¼ì˜! ìµœìƒë‹¨ ë£¨íŠ¸ë…¸ë“œ ë°˜ë“œì‹œ ì—…ë°ì´íŠ¸í• ê²ƒ -> top queriesê°€ ì €ì¥ë˜ë¯€ë¡œ)           
```

An example of how the update operation works(Figure 13-13)
```  
  Search query â€œbeerâ€
  value 10 -> 30(its ancestors)
```
![fg13-13](img/fg13-13.jpg)


#### Delete
```
We have to remove hateful, violent, sexually explicit, or dangerous autocomplete suggestions.
(íŠ¸ë¼ì´ ìºì‹œ ì•ì— í•„í„° ë°°ì¹˜í•˜ì—¬ ë¶€ì ì ˆí•œ ì‘ë‹µ ë¦¬í„´ ë°©ì§€)
Having a filter layer gives us the flexibility of removing results based on different filter rules.
Unwanted suggestions are removed physically from the database asynchronically so the correct data set will be used to build trie in the next update cycle.
```
![fg13-14](img/fg13-14.jpg)



### (5) Scale the storage 
```

- Since English is the only supported language, a naive way to shard is based on the first character.(ì˜ì–´ê¸°ì¤€, ì²« ê¸€ì ê¸°ì¤€ìœ¼ë¡œ ìƒ¤ë”©ë°©ì•ˆ)

  Here are some examples.
    â€¢ If we need two servers for storage, we can store queries starting with â€˜aâ€™ to â€˜mâ€™ on the first server, and â€˜nâ€™ to â€˜zâ€™ on the second server.
      2ëŒ€ ì„œë²„ í•„ìš” ì‹œ, 'a'~'m' ì‹œì‘í•˜ëŠ” ê²€ìƒ‰ì–´ ->  1st server ì €ì¥ / the rest -> 2nd server ì €ì¥
    â€¢ If we need three servers, we can split queries into â€˜aâ€™ to â€˜iâ€™, â€˜jâ€™ to â€˜râ€™ and â€˜sâ€™ to â€˜zâ€™.
      3ëŒ€ ì„œë²„ í•„ìš”ì‹œ, â€˜aâ€™ ~â€˜iâ€™ /  â€˜jâ€™ ~ â€˜râ€™ / â€˜sâ€™ ~ â€˜zâ€™
    â€¢ Following this logic, we can split queries up to 26 servers because there are 26 alphabetic characters in English.
      í•´ë‹¹ ë°©ì•ˆì€ ì„œë²„ 26ê°œë¡œ ì œí•œ( ì•ŒíŒŒë²³26 ë•Œë¬¸)
    â€¢ To store data beyond 26 servers, we can shard on the second or even at the third level.
      26ê°œ ì´ìƒ ì‹œ ìƒ¤ë”©ì˜ ê³„ì¸µí™”í•„ìš” ì˜ˆ í•˜ë‚˜ì˜ â€˜aâ€™ëŒ€í•´ -> 4 servers ë‚˜ëˆ”: â€˜aa-agâ€™, â€˜ah-anâ€™, â€˜ao-auâ€™, â€˜av-azâ€™.

- Historical data distribution pattern (ê³¼ê±° query íŒ¨í„´ ì ìš© ë°©ì•ˆ) ì•„ë˜ ê·¸ë¦¼ì°¸ì¡°
     â€¢ The shard map manager maintains a lookup database for identifying where rows should be stored.
       shard map manager ê²€ìƒ‰ì–´ì™€ ì €ì¥ëœ ì„œë²„ ë§¤í•‘ê´€ê³„ ê´€ë¦¬í•˜ê³  íŒ¨í„´ì— ë”°ë¥¸ ë°°ë¶„ ìƒ¤ë”©ë°©ì‹
        For example,
                  the number of â€˜sâ€™ ì‹œì‘ì–´
                        =
                  the number of â€˜uâ€™, â€˜vâ€™, â€˜wâ€™, â€˜xâ€™, â€˜yâ€™ and â€˜zâ€™ ì‹œì‘ì–´
        then,
               Two shards â€˜sâ€™ / â€˜uâ€™ ~â€˜zâ€™
```

![fg13-15](img/fg13-15.jpg)


## Step 4 - Wrap up
Some follow up questions

> How do you extend your design to support multiple languages?
```
Unicode ì½”ë“œ ê³ ë ¤í•˜ì—¬ ì ìš©í•œë‹¤.
To support other non-English queries, we store Unicode characters in trie nodes.
Unicode: an encoding standard covering all the characters(world, modern and ancient)
```

> What if top search queries in one country are different from others?
```
ë‹¤ë¥¸ êµ­ê°€ì— ë”°ë¥¸ ë‹¤ë¥¸ íŠ¸ë¼ì´ í•„ìš”. ì‘ë‹µì‹œê°„ í–¥ìƒìœ„í•´ CDN í™œìš©í•œë‹¤.
In this case, we might build different tries for different countries. 
To improve the response time, we can store tries in CDNs.
      A CDN is a network of geographically dispersed servers used to deliver static content.
      CDN servers cache static content like images, videos, CSS, JavaScript files, etc.
      (CDN: ì‚¬ìš©ìì—ê²Œ ì›¹ ì½˜í…ì¸ ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì œê³µí•  ìˆ˜ ìˆëŠ” ì„œë²„ì˜ ë¶„ì‚° ë„¤íŠ¸ì›Œí¬)
```

> How can we support the trending (real-time) search queries?
```
Assuming a news event breaks out, a search query suddenly becomes popular. ê°‘ìê¸° í•œ ì§ˆì˜ì–´ê°€ í­ë°œì  ì¸ê¸° ì‹œ
Our original design will not work because: ê¸°ì¡´ ë°©ì•ˆìœ¼ë¡œ ëŒ€ì‘ ì–´ë µë‹¤. ì™œëƒë©´
  â€¢ Offline workers are not scheduled to update the trie yet because this is scheduled to run on weekly basis. (ì£¼ë³„ê¸°ì¤€ì„œë²„ì˜ ì—…ë°ì´íŠ¸ë¡œ ì¸í•œ ë¯¸ë°˜ì˜)
  â€¢ Even if it is scheduled, it takes too long to build the trie. (ë°˜ì˜ëœë‹¤í•´ë„ ì˜¤ë˜ê±¸ë¦¼)

ì´ëŠ” ìŠ¤ì½¥ì´ ë²—ì–´ë‚˜ì§€ë§Œ ì•„ë˜ ë‚´ìš©ìœ¼ë¡œ ìƒê°í•´ë³¼ìˆ˜ìˆë‹¤. A few ideas for a real-time search autocomplete

â€¢ Reduce the working data set by sharding. (ìƒ¤ë”© - ì‘ì—…ë°ì´í„°ì…‹ ì¶•ì†Œ)
â€¢ Change the ranking model and assign more weight to recent search queries.(ìµœê·¼ê²€ìƒ‰ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬)
â€¢ Data may come as streams, so we do not have access to all the data at once. ???
      Streaming data: generated data continuously.
      Stream processing requires a different set of systems:
                Apache Hadoop MapReduce [6], Apache Spark Streaming [7], Apache Storm [8], Apache Kafka [9], etc.
```

